{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# world_model_v6 (with XGBoost)\n",
    "May 7, 2025 (Updated with XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <54A1AE05-1E14-3DA2-A8D0-062134694298> /Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/lib-dynload/../../libomp.dylib' (no such file), '/Users/albertolandi/anaconda3/envs/IR2/bin/../lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ridge \u001b[38;5;66;03m# Keep for comparison or if needed later\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[0;32m~/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/core.py:295\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/core.py:257\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    256\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[1;32m    272\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    274\u001b[0m     libver \u001b[38;5;241m=\u001b[39m _lib_version(lib)\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <54A1AE05-1E14-3DA2-A8D0-062134694298> /Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/Users/albertolandi/anaconda3/envs/IR2/lib/python3.10/lib-dynload/../../libomp.dylib' (no such file), '/Users/albertolandi/anaconda3/envs/IR2/bin/../lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge # Keep for comparison or if needed later\n",
    "import xgboost as xgb \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# xgb.DMatrix.set_random_state(42) # For older versions, newer ones use np.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath=\"../dataset/dataset_v4.txt\"):\n",
    "    \"\"\"Loads the dataset using pandas.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        df = df.dropna()\n",
    "        print(f\"Shape after dropping NaNs: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the dataset\n",
    "# Create a dummy dataset_v4.txt for testing if it doesn't exist\n",
    "dummy_data = \"\"\"distance_red_init,angle_red_init,distance_green_init,angle_green_init,distance_blue_init,angle_blue_init,rSpeed,lSpeed,distance_red_final,angle_red_final,distance_green_final,angle_green_final,distance_blue_final,angle_blue_final\n",
    "847.18237502475,135.0077102870207,848.4285070131579,44.91001799841399,848.6299022262742,-134.9082395663736,30,17,919.3516568234484,169.5845864368397,744.6266372511928,77.89688504365728,957.4204577425843,-110.95023009036865\n",
    "919.3516568234484,169.5845864368397,744.6266372511928,77.89688504365728,957.4204577425843,-110.95023009036865,-6,25,963.2935290670038,108.50162756834925,698.9871122863157,17.50247248666159,1010.68276681774,-176.66445630856867\n",
    "963.2935290670038,108.50162756834925,698.9871122863157,17.50247248666159,1010.68276681774,-176.66445630856867,-9,-6,932.4764088112901,95.43414527435988,692.6089322190409,1.4040251940733697,1010.456972359338,171.60189312665523\n",
    "932.4764088112901,95.43414527435988,692.6089322190409,1.4040251940733697,1010.456972359338,171.60189312665523,24,-23,902.6519916326283,-177.68212789984284,698.1799037485613,86.04292804685633,1000.9520797712298,-99.71404373987178\n",
    "902.6519916326283,-177.68212789984284,698.1799037485613,86.04292804685633,1000.9520797712298,-99.71404373987178,-17,-23,923.7137529546012,-152.87166980372595,793.969133781329,118.83300496617363,909.4480927449757,-71.09601015189716\n",
    "\"\"\"\n",
    "dataset_dir = \"../dataset\"\n",
    "dataset_path = os.path.join(dataset_dir, \"dataset_v4.txt\")\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "if not os.path.exists(dataset_path):\n",
    "    with open(dataset_path, \"w\") as f:\n",
    "        f.write(dummy_data)\n",
    "    print(f\"Created dummy dataset at {dataset_path}\")\n",
    "\n",
    "dataframe = load_data(filepath=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataframe is not None:\n",
    "    dataframe.info()\n",
    "else:\n",
    "    print(\"DataFrame is None, skipping info().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"Separates features (X) and target variables (Y).\"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    # Input Features: initial state (6) + action (2) = 8 features\n",
    "    X = df.iloc[:, :8].values\n",
    "    # Target Variables: final state (6) = 6 features\n",
    "    Y = df.iloc[:, 8:].values\n",
    "    print(f\"Features (X) shape: {X.shape}\")\n",
    "    print(f\"Targets (Y) shape: {Y.shape}\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "    if X is None or Y is None:\n",
    "        return None, None, None, None\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare Data\n",
    "if dataframe is not None:\n",
    "    X, Y = prepare_data(dataframe)\n",
    "else:\n",
    "    X, Y = None, None\n",
    "    print(\"Skipping data preparation as dataframe is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"Scales input features using StandardScaler.\"\"\"\n",
    "    if X_train is None or X_test is None:\n",
    "        return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    # Fit scaler ONLY on training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    # Transform both train and test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Features scaled.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler # Return scaler to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split Data\n",
    "if X is not None and Y is not None:\n",
    "    X_train, X_test, Y_train, Y_test = split_data(X, Y)\n",
    "    # 4. Scale Features (Important!)\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "else:\n",
    "    X_train, X_test, Y_train, Y_test = None, None, None, None\n",
    "    X_train_scaled, X_test_scaled, scaler = None, None, None\n",
    "    print(\"Skipping data splitting and scaling as X or Y is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Ridge Regression (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_regression(X_train, Y_train):\n",
    "    \"\"\"Trains a Ridge Regression model with hyperparameter optimization using\n",
    "    GridSearchCV.\"\"\"\n",
    "    if X_train is None or Y_train is None:\n",
    "        print(\"Skipping Ridge training as data is None.\")\n",
    "        return None\n",
    "    print(\"Training Ridge Regression model with GridSearchCV...\")\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 1000000.0],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "    }\n",
    "    ridge = Ridge(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=ridge,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    print(\"\\nGridSearchCV Complete for Ridge.\")\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score (negative MSE): {grid_search.best_score_:.4f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# # Example usage for Ridge (commented out if focusing on XGBoost)\n",
    "# if X_train_scaled is not None and Y_train is not None:\n",
    "#     print(\"\\n--- Training Ridge Model ---\")\n",
    "#     world_model_ridge = train_ridge_regression(X_train_scaled, Y_train)\n",
    "# else:\n",
    "#     world_model_ridge = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_regression(X_train, Y_train):\n",
    "    \"\"\"Trains an XGBoost Regressor model with hyperparameter optimization using GridSearchCV.\"\"\"\n",
    "    if X_train is None or Y_train is None:\n",
    "        print(\"Skipping XGBoost training as data is None.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"Training XGBoost Regressor model with GridSearchCV...\")\n",
    "\n",
    "    # Define the parameter grid for XGBoost\n",
    "    # This is a starting grid. You might want to expand or refine it.\n",
    "    # For faster initial runs, you can reduce the number of options or use a smaller cv.\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200, 300], # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage\n",
    "        'max_depth': [3, 5, 7], # Maximum depth of a tree\n",
    "        # 'subsample': [0.7, 0.8, 1.0], # Subsample ratio of the training instance\n",
    "        # 'colsample_bytree': [0.7, 0.8, 1.0], # Subsample ratio of columns when constructing each tree\n",
    "        # 'gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition\n",
    "        # 'reg_alpha': [0, 0.01, 0.1], # L1 regularization\n",
    "        # 'reg_lambda': [0.1, 1, 10] # L2 regularization (XGBoost's default is 1 for lambda)\n",
    "    }\n",
    "\n",
    "    # Create the XGBoost regressor model\n",
    "    # objective='reg:squarederror' is common for regression\n",
    "    # XGBoost can use multiple cores for training individual models via n_jobs in its constructor\n",
    "    # For multi-output regression, XGBoost handles it natively.\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    # cv=3 for quicker search, use cv=5 for more robust results\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring='neg_mean_squared_error', # Optimize for lower MSE\n",
    "        cv=3, # Reduced for speed in this example; use 5 for better tuning\n",
    "        n_jobs=-1, # Use all available CPU cores for GridSearchCV's parallel fits\n",
    "        verbose=2 # Higher verbosity to see progress\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV on the training data\n",
    "    grid_search_xgb.fit(X_train, Y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding score\n",
    "    print(\"\\nGridSearchCV Complete for XGBoost.\")\n",
    "    print(f\"Best parameters found: {grid_search_xgb.best_params_}\")\n",
    "    print(f\"Best cross-validation score (negative MSE): {grid_search_xgb.best_score_:.4f}\")\n",
    "\n",
    "    # Return the best model found by GridSearchCV\n",
    "    return grid_search_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "if X_train_scaled is not None and Y_train is not None:\n",
    "    print(\"\\n--- Training XGBoost Model ---\")\n",
    "    world_model_xgb = train_xgboost_regression(X_train_scaled, Y_train)\n",
    "else:\n",
    "    world_model_xgb = None\n",
    "    print(\"Skipping XGBoost model training call.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluates the model using MAE, MSE, and RMSE.\"\"\"\n",
    "    if model is None or X_test is None or Y_test is None:\n",
    "        print(f\"Skipping evaluation for {model_name} as model or data is None.\")\n",
    "        return None, None\n",
    "        \n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mse) # Root Mean Squared Error\n",
    "\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "    # Optional: Print metrics per output feature\n",
    "    print(\"\\nMAE per output feature:\")\n",
    "    output_features = [\n",
    "        'dist_red_final', 'angle_red_final', 'dist_green_final',\n",
    "        'angle_green_final', 'dist_blue_final', 'angle_blue_final'\n",
    "    ]\n",
    "    for i, name in enumerate(output_features):\n",
    "        # Ensure Y_test and Y_pred are 2D, even if only one sample was predicted\n",
    "        y_test_col = Y_test[:, i] if Y_test.ndim > 1 else Y_test\n",
    "        y_pred_col = Y_pred[:, i] if Y_pred.ndim > 1 else Y_pred\n",
    "        \n",
    "        mae_feature = mean_absolute_error(y_test_col, y_pred_col)\n",
    "        print(f\"  {name}: {mae_feature:.4f}\")\n",
    "    \n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost model\n",
    "if world_model_xgb is not None and X_test_scaled is not None and Y_test is not None:\n",
    "    print(\"\\n--- Evaluating XGBoost Model ---\")\n",
    "    xgb_mae, xgb_mse = evaluate_model(world_model_xgb, X_test_scaled, Y_test, model_name=\"XGBoost\")\n",
    "    if xgb_mae is not None:\n",
    "        print(f\"XGBoost Test RMSE: {np.sqrt(xgb_mse):.4f}\")\n",
    "else:\n",
    "    print(\"Skipping XGBoost model evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_scaler(model, scaler, model_filename=\"world_model.joblib\", scaler_filename=\"scaler.joblib\"):\n",
    "    \"\"\"Saves the trained model and scaler to disk.\"\"\"\n",
    "    if model is None or scaler is None:\n",
    "        print(\"Skipping saving model/scaler as one of them is None.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        model_dir = \"../src/models\" # Relative to script location\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        model_path = os.path.join(model_dir, model_filename)\n",
    "        scaler_path = os.path.join(model_dir, scaler_filename)\n",
    "\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model/scaler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the XGBoost model and scaler\n",
    "if world_model_xgb is not None and scaler is not None:\n",
    "    print(\"\\n--- Saving XGBoost Model and Scaler ---\")\n",
    "    save_model_and_scaler(world_model_xgb, scaler, \n",
    "                          model_filename=\"xgb_world_model_v6.joblib\", \n",
    "                          scaler_filename=\"xgb_scaler_v6.joblib\") # Use distinct names\n",
    "else:\n",
    "    print(\"Skipping XGBoost model saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Example Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction (how you'd use it later)\n",
    "def example_prediction(model, X_test_original, Y_test_original, scaler_obj, model_name=\"Model\"):\n",
    "    if model is None or X_test_original is None or Y_test_original is None or scaler_obj is None:\n",
    "        print(f\"Skipping example prediction for {model_name} as essential components are missing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Example Prediction ({model_name}) ---\")\n",
    "\n",
    "    # Take the first sample from the original (unscaled) test set\n",
    "    sample_X_orig = X_test_original[0].reshape(1, -1)\n",
    "    sample_Y_actual = Y_test_original[0]\n",
    "\n",
    "    # Scale the sample using the *saved* (or current) scaler\n",
    "    sample_X_scaled = scaler_obj.transform(sample_X_orig)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    sample_Y_pred = model.predict(sample_X_scaled)\n",
    "\n",
    "    print(f\"Input State + Action (Original): {sample_X_orig[0]}\")\n",
    "    print(f\"Input State + Action (Scaled):   {sample_X_scaled[0]}\")\n",
    "    print(f\"Actual Final State:              {sample_Y_actual}\")\n",
    "    print(f\"Predicted Final State:           {sample_Y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction with XGBoost model\n",
    "if world_model_xgb and X_test is not None and Y_test is not None and scaler is not None:\n",
    "    example_prediction(world_model_xgb, X_test, Y_test, scaler, model_name=\"XGBoost\")\n",
    "else:\n",
    "    print(\"Skipping XGBoost example prediction due to missing components.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
